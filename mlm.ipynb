{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMup/uW99r5l9jqsvlbamUD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nepomucenoc/MLM/blob/main/mlm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Masked Language Model\n",
        " In machine learning, MLM stands for \"Masked Language Model,\" which is a technique used in Natural Language Processing (NLP) to train language models by randomly masking certain words within a sentence and then asking the model to predict what those masked words are based on the surrounding context; this helps the model learn deep semantic relationships between words and understand the overall meaning of a sentence, making it particularly useful for tasks like text classification, question answering, and text generation."
      ],
      "metadata": {
        "id": "Z1tonKBh5HBL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "pAlcFOpapycD",
        "outputId": "cdccbf75-6ac1-4685-c9fd-9d329164e9f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.48.3)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.3.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.17.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.28.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.10.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.13)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "BertForMaskedLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
            "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
            "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
            "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcarolina-nep\u001b[0m (\u001b[33mcarolina-nep-ufc\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.7"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250306_134403-s87l8u15</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/carolina-nep-ufc/huggingface/runs/s87l8u15' target=\"_blank\">./results</a></strong> to <a href='https://wandb.ai/carolina-nep-ufc/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/carolina-nep-ufc/huggingface' target=\"_blank\">https://wandb.ai/carolina-nep-ufc/huggingface</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/carolina-nep-ufc/huggingface/runs/s87l8u15' target=\"_blank\">https://wandb.ai/carolina-nep-ufc/huggingface/runs/s87l8u15</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='939' max='939' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [939/939 9:44:50, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.153890</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.528100</td>\n",
              "      <td>0.126949</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.528100</td>\n",
              "      <td>0.118201</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Text: the quick brown fox jumps over the little dog.\n"
          ]
        }
      ],
      "source": [
        "# Install dependencies\n",
        "!pip install transformers datasets\n",
        "\n",
        "import torch\n",
        "from transformers import BertTokenizer, BertForMaskedLM, Trainer, TrainingArguments\n",
        "from transformers import pipeline\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Loading the Tokenizer and the BERT Pre-Trained Model\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Loading data - using a simple HuggingFace dataset for NLP\n",
        "dataset = load_dataset('imdb', split='train[:5%]')  # Apenas uma amostra para simplificaÃ§Ã£o\n",
        "\n",
        "# Tokenizing the texts\n",
        "def tokenize_function(examples):\n",
        "    # Tokenizing and Preparing Data for Masked Language Modeling\n",
        "    encodings = tokenizer(examples['text'], padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
        "    encodings['labels'] = encodings['input_ids'].detach().clone()  # Using input_ids themselves as labels\n",
        "    # Applying a random mask to the texts for MLM\n",
        "    # Creating a mask at random positions\n",
        "    # Setting 15% of tokens to be masked\n",
        "    for i in range(len(encodings['input_ids'])):\n",
        "        # Creating a random mask (just as a simple example)\n",
        "        rand = torch.rand(encodings['input_ids'].shape[1])\n",
        "        mask_idx = rand < 0.15\n",
        "        encodings['input_ids'][i, mask_idx] = tokenizer.mask_token_id  # Replaces token with mask\n",
        "    return encodings\n",
        "\n",
        "# Apply tokenization to the entire dataset\n",
        "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "# Now tokenized dataset should contain the columns: 'input_ids', 'attention_mask', 'labels'\n",
        "\n",
        "# Defining training parameters\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',          # where the results are save\n",
        "    evaluation_strategy=\"epoch\",     # evaluate each epoch\n",
        "    learning_rate=2e-5,              # Learning rate\n",
        "    per_device_train_batch_size=4,   # Training batch size (reduced)\n",
        "    num_train_epochs=3,              # Number of training epochs\n",
        "    weight_decay=0.01,               # Weight decay\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets,  # Here we pass the tokenized dataset\n",
        "    eval_dataset=tokenized_datasets    # For the evaluation, we also used the tokenized dataset\n",
        ")\n",
        "\n",
        "# Start train\n",
        "trainer.train()\n",
        "\n",
        "# Text Generation Pipeline with Fine-Tuning\n",
        "text_generator = pipeline('fill-mask', model=model, tokenizer=tokenizer)\n",
        "\n",
        "# Testing the model with an example sentence\n",
        "input_text = \"The quick brown fox jumps over the [MASK] dog.\"\n",
        "output = text_generator(input_text)\n",
        "\n",
        "# Displaying the generated output\n",
        "print(f\"Generated Text: {output[0]['sequence']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Examples test"
      ],
      "metadata": {
        "id": "0GtTP6qf9x-8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Testando o modelo com uma frase de exemplo\n",
        "input_text = \"The hair of that girl is [MASK].\"\n",
        "output = text_generator(input_text)\n",
        "\n",
        "# Exibindo a saÃ­da gerada\n",
        "print(f\"Generated Text: {output[0]['sequence']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_BuKf7ik31DC",
        "outputId": "efa27d63-7641-4a84-d104-bae7d565ea64"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Text: the hair of that girl is black.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Testando o modelo com outra frase de exemplo\n",
        "input_text = \"I love [MASK].\"\n",
        "output = text_generator(input_text)\n",
        "\n",
        "# Exibindo a saÃ­da gerada\n",
        "print(f\"Generated Text: {output[0]['sequence']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P7M65JlV83o3",
        "outputId": "bd2979b5-d470-4128-81c3-843c74546881"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Text: i love you.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Save Model"
      ],
      "metadata": {
        "id": "MZIzdCOr9Q6B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained('./fine_tuned_model')\n",
        "\n",
        "# save tokenizer\n",
        "tokenizer.save_pretrained('./fine_tuned_model')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P0ur5f5V39NV",
        "outputId": "2ef0dd76-e2e7-4a95-c4da-f314266a4314"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('./fine_tuned_model/tokenizer_config.json',\n",
              " './fine_tuned_model/special_tokens_map.json',\n",
              " './fine_tuned_model/vocab.txt',\n",
              " './fine_tuned_model/added_tokens.json')"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Load the model and tokenizer later:"
      ],
      "metadata": {
        "id": "DiziiiRM9YvA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertForMaskedLM, BertTokenizer\n",
        "\n",
        "# Load model and tokenizer\n",
        "model = BertForMaskedLM.from_pretrained('./fine_tuned_model')\n",
        "tokenizer = BertTokenizer.from_pretrained('./fine_tuned_model')"
      ],
      "metadata": {
        "id": "okqD5zFw50Kr"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Save checkpoints during training:"
      ],
      "metadata": {
        "id": "3wRYf-xL9iNY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',          # Onde os resultados sÃ£o salvos\n",
        "    save_steps=500,                  # Salvar checkpoints a cada 500 passos\n",
        "    save_total_limit=2,              # Manter apenas 2 checkpoints mais recentes\n",
        "    evaluation_strategy=\"epoch\",     # AvaliaÃ§Ã£o a cada Ã©poca\n",
        "    learning_rate=2e-5,              # Taxa de aprendizado\n",
        "    per_device_train_batch_size=4,   # Tamanho do lote de treinamento\n",
        "    num_train_epochs=3,              # NÃºmero de Ã©pocas de treinamento\n",
        "    weight_decay=0.01,               # Decaimento de peso\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "smuXvZu66Hm1",
        "outputId": "dec071a1-295f-4f08-ce84-e502389483b6"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Save the template to Google Drive (optional):"
      ],
      "metadata": {
        "id": "fPo7xaEO9oNx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Save in Google Drive\n",
        "model.save_pretrained('/content/drive/MyDrive/DataScience/MLM/fine_tuned_model')\n",
        "tokenizer.save_pretrained('/content/drive/MyDrive/DataScience/MLM/fine_tuned_model')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j6A7f5GN6TGD",
        "outputId": "79b36f3f-59b2-4883-aeb5-dc48b4102605"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('/content/drive/MyDrive/DataScience/MLM/fine_tuned_model/tokenizer_config.json',\n",
              " '/content/drive/MyDrive/DataScience/MLM/fine_tuned_model/special_tokens_map.json',\n",
              " '/content/drive/MyDrive/DataScience/MLM/fine_tuned_model/vocab.txt',\n",
              " '/content/drive/MyDrive/DataScience/MLM/fine_tuned_model/added_tokens.json')"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    }
  ]
}